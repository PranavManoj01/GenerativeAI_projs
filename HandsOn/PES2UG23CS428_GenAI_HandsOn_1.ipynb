{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c73ef-fabc-4fab-ab4d-d275ee320356",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name: Pranav Manoj\n",
    "SRN: PES2UG23CS428\n",
    "Date : 27/01/26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410223dc-1196-403e-9aef-f298e4bf5d21",
   "metadata": {},
   "source": [
    "To compare the difference between various models like BERT, RoBERTa and BART by testing them against tasks they were made for and tasks that they were not made for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97e989e-5d89-4eec-aefd-eb03652b5384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pranav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e89a938-8a7c-40a6-8d10-d4d6a2584b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "models={'BERT':'bert-base-uncased','RoBERTa':'roberta-base','BART':'facebook/bart-base'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281991b4-ba9f-46cc-afa6-c2a4c7317d77",
   "metadata": {},
   "source": [
    "Experiment 1 : \n",
    "Task : To generate text using the prompt \"The future of articial intelligence is\"\n",
    "\n",
    "Hypothesis : BERT and RoBERTa are encoder-only models trained for understanding rather than autoregressive generation. They lack a causal attention mechanism that allows to generate tokens sequentially.\n",
    "\n",
    "Expectations : BERT and RoBERTa will fail and BART might work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bddf92bd-4fab-462a-bb2c-c759c296537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPERIMENT 1 : TEXT GENERATION\n",
      "Testing: BERT (bert-base-uncased)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|██| 202/202 [00:00<00:00, 309.23it/s, Materializing param=cls.predictions.transform.dense.weight]\n",
      "BertLMHeadModel LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Passing `generation_config` together with generation-related arguments=({'num_return_sequences', 'max_length'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Future of Artificial Intelligence is. the ( / or or. him the or or or so we and and and and and and and to he do so jefferson was music for pit how it it it it is on rush to colt are as housing'''''( \". more actual real and and and and'' -, \". ( \" ( \" he hell jd in general like change change and - - ( ( \". the and and the circlesing, \" ( \" ( \" ( \" ( ) ) ( ; \". it it it it it though \" ) that and and \" \" ( \" ( \" ( ). that beautiful mother in them actually so their \" so so i as their on complete ( \" ( \" \". it it it was no or song to was the the and thele [ [ [ [ [ [, have with me the so girls whenever time as my the the ring.. the as or that. ( \" \". it on he often so girls mothers \". it and and and and and and and and and - \" other is up us and in scotland they for. the the the and and and and and more more actually or general it it was men men, of his \" \" and and the seen or and - ( or it being\n",
      "Testing: RoBERTa (roberta-base)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|█| 202/202 [00:00<00:00, 366.85it/s, Materializing param=roberta.encoder.layer.11.output.dense.we\n",
      "RobertaForCausalLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Future of Artificial Intelligence is.\n",
      "Testing: BART (facebook/bart-base)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 159/159 [00:00<00:00, 294.90it/s, Materializing param=model.decoder.layers.5.self_attn_layer_n\n",
      "This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are absent from the checkpoint, and we could not find another related tied weight for those keys\n",
      "BartForCausalLM LOAD REPORT from: facebook/bart-base\n",
      "Key                                                           | Status     | \n",
      "--------------------------------------------------------------+------------+-\n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.weight | UNEXPECTED | \n",
      "shared.weight                                                 | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.weight   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.bias     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.bias   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.bias       | UNEXPECTED | \n",
      "encoder.layernorm_embedding.bias                              | UNEXPECTED | \n",
      "encoder.embed_positions.weight                                | UNEXPECTED | \n",
      "encoder.layernorm_embedding.weight                            | UNEXPECTED | \n",
      "lm_head.weight                                                | MISSING    | \n",
      "model.decoder.embed_tokens.weight                             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Future of Artificial Intelligence is* Comic sauces latest latest Broadcasting�217 dignity latest latestautical latest Apex fingertipsDonnellDonnell Hue Hueシャuthor latest behestewayHyper 215 CHR 215 laps protests flamingautical preserved tragically latest disg latest fingertips latest Hearthstone latest blinking tim latest 215 NXT latest 215 fingertips hugehighlyWI trians MissER fingertips plethora 215 Cheney cheapest CHRER Marshal 215 Marshal Marshal Marshal 215 roads climate 215 215 absolute647 215 215Ord cheapest tragically tragically tragically Hue Hue tragicallyrie linguistic tragicallytechnical Wyr17technicalians tragicallytechnical tragically647 graduate tragically Madrid tragicallytechnical Fridaystechnical Madrid Winginary tragically disband tragically tragicallyvectorvectorabiliatechnical Madrid tragically وabiliatechnical tragically Madridicflationflation tragically grindingidth tragicallyvector Cheneyvector tragicallyflationImAndrewflationicianstechnical tragicallyians tragically FTC tragicallyyoung tragically tragically Madridyoung/(ians renegotiProsvectorians tragically tragicallyic tragicallyinnon FTC tragicallyvector tragicallyiciansyoung tragically disband commissionedvectorvector proportional proportional tragically Surviv Madrid Madrid THEIR tragicallyvector Madridvector و tragically proportional tragically Madrid tweaks tragicallyvector proportional climateyoung tragicallyvector}{technical Madrid Madrid regain coupicinnon}{ic Madrid Miss Madrid constructionomer proportional flaming Madridbroken regainomer proportional therebyicic Madridic Madridomer tragicallyic regainic constructionicomerbrokenic AB construction verified Chaoic proportionalic construction Emmanuel construction tragically construction}{ic construction construction construction therebybroken construction\n",
      "\n",
      "EXPERIMENT 1 COMPLETE\n"
     ]
    }
   ],
   "source": [
    "prompt=\"Future of Artificial Intelligence is\"\n",
    "print()\n",
    "print(\"EXPERIMENT 1 : TEXT GENERATION\")\n",
    "for model_name,model_id in models.items():\n",
    "    print(f\"Testing: {model_name} ({model_id})\")\n",
    "    try:\n",
    "        generator = pipeline('text-generation', model=model_id)\n",
    "        result = generator(prompt, max_length=30, num_return_sequences=1)\n",
    "        print(f\"Generated text: {result[0]['generated_text']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ FAILED\")\n",
    "        print(f\"Error: {type(e).__name__}\")\n",
    "        print(f\"Message: {str(e)[:200]}...\" if len(str(e)) > 200 else f\"Message: {str(e)}\")\n",
    "        print(f\"\\nExplanation: {model_name} is an encoder-only model without autoregressive generation capability.\")\n",
    "print()\n",
    "print(\"EXPERIMENT 1 COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d35dc3-e49f-4e4f-a61c-aee94de9d6d8",
   "metadata": {},
   "source": [
    "Experiment 2 : Masked Language Modeling \n",
    "\n",
    "Task : Predict the missing word in:\"The goal of Generative AI is to [MASK] new content.\"\n",
    "\n",
    "Hypothesis: BERT should work well(this is its primary training task), RoBERTa should work well (optimized version of BERT), BART: Should work well (trained with denoising objectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b443c54a-d0da-41fa-8184-2f1345bfdd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 2:MASKED LANGUAGE MODELLING (FILL MASK)\n",
      "Testing: BERT (bert-base-uncased)\n",
      "Input text: 'The goal of Generative AI is to [MASK] new content.'\n",
      "Mask token used: [MASK]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██| 202/202 [00:00<00:00, 357.16it/s, Materializing param=cls.predictions.transform.dense.weight]\n",
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "\n",
      "  1. 'create' (score: 0.5397)\n",
      "  2. 'generate' (score: 0.1558)\n",
      "  3. 'produce' (score: 0.0541)\n",
      "  4. 'develop' (score: 0.0445)\n",
      "  5. 'add' (score: 0.0176)\n",
      "Testing: RoBERTa (roberta-base)\n",
      "Input text: 'The goal of Generative AI is to <mask> new content.'\n",
      "Mask token used: <mask>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 202/202 [00:00<00:00, 316.63it/s, Materializing param=roberta.encoder.layer.11.output.dense.we\n",
      "RobertaForMaskedLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "\n",
      "  1. 'generate' (score: 0.3711)\n",
      "  2. 'create' (score: 0.3677)\n",
      "  3. 'discover' (score: 0.0835)\n",
      "  4. 'find' (score: 0.0213)\n",
      "  5. 'provide' (score: 0.0165)\n",
      "Testing: BART (facebook/bart-base)\n",
      "Input text: 'The goal of Generative AI is to [MASK] new content.'\n",
      "Mask token used: [MASK]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█████████████████████| 259/259 [00:00<00:00, 344.15it/s, Materializing param=model.shared.weight]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: PipelineException\n",
      "Message: No mask_token (<mask>) found on the input\n",
      "EXPERIMENT 2 COMPLETE\n"
     ]
    }
   ],
   "source": [
    "print(\"EXPERIMENT 2:MASKED LANGUAGE MODELLING (FILL MASK)\")\n",
    "masked_text = \"The goal of Generative AI is to <mask> new content.\"\n",
    "mask_token = \"<mask>\"\n",
    "for model_name, model_id in models.items():\n",
    "    print(f\"Testing: {model_name} ({model_id})\")\n",
    "    if 'roberta' in model_id.lower():\n",
    "        masked_text = \"The goal of Generative AI is to <mask> new content.\"\n",
    "        mask_token = \"<mask>\"\n",
    "    else:\n",
    "        masked_text = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "        mask_token = \"[MASK]\"\n",
    "    print(f\"Input text: '{masked_text}'\")\n",
    "    print(f\"Mask token used: {mask_token}\\n\")\n",
    "    try:\n",
    "        fill_mask = pipeline('fill-mask', model=model_id)\n",
    "        results = fill_mask(masked_text)\n",
    "        print(f\"Top 5 predictions:\\n\")\n",
    "        for i, result in enumerate(results[:5], 1):\n",
    "            print(f\"  {i}. '{result['token_str'].strip()}' (score: {result['score']:.4f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {type(e).__name__}\")\n",
    "        print(f\"Message: {str(e)[:200]}...\" if len(str(e)) > 200 else f\"Message: {str(e)}\")\n",
    "print(\"EXPERIMENT 2 COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f7b53-c48c-4506-8d0f-a97844556525",
   "metadata": {},
   "source": [
    "Experiment 3 : \n",
    "Task :Answer the question \"What are the risks?\" based on context\n",
    "\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "\n",
    "Hypothesis: These are the baseline models. Question answering requires extracting spans from context, which encoder models can do architecturally, but performance depends on fine-tuning.\n",
    "\n",
    "Expected results:\n",
    "BERT: May work but with poor quality, RoBERTa : Similar to BERT, BART: May struggle more as it's designed for generation, not extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e9e5c5-2761-404b-91c7-8ce2057c0dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 3: QUESTION ANSWERING\n",
      "\n",
      "Question: 'What are the risks?'\n",
      "Context: 'Generative AI poses significant risks such as hallucinations, bias, and deepfakes.'\n",
      "\n",
      "Testing: BERT (bert-base-uncased)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 197/197 [00:01<00:00, 103.99it/s, Materializing param=bert.encoder.layer.11.output.dense.weigh\n",
      "BertForQuestionAnswering LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "qa_outputs.bias                            | MISSING    | \n",
      "qa_outputs.weight                          | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'AI poses significant risks such as hallucinations'\n",
      "Confidence score: 0.0087\n",
      "Start position: 11\n",
      "End position: 60\n",
      "\n",
      " Note: Low confidence score suggests the model is not well-suited for this task without fine-tuning.\n",
      "Testing: RoBERTa (roberta-base)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 197/197 [00:01<00:00, 121.24it/s, Materializing param=roberta.encoder.layer.11.output.dense.we\n",
      "RobertaForQuestionAnswering LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "qa_outputs.bias                 | MISSING    | \n",
      "qa_outputs.weight               | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'Generative'\n",
      "Confidence score: 0.0146\n",
      "Start position: 0\n",
      "End position: 10\n",
      "\n",
      " Note: Low confidence score suggests the model is not well-suited for this task without fine-tuning.\n",
      "Testing: BART (facebook/bart-base)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█████████████████████| 259/259 [00:02<00:00, 112.22it/s, Materializing param=model.shared.weight]\n",
      "BartForQuestionAnswering LOAD REPORT from: facebook/bart-base\n",
      "Key               | Status  | \n",
      "------------------+---------+-\n",
      "qa_outputs.bias   | MISSING | \n",
      "qa_outputs.weight | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ', bias'\n",
      "Confidence score: 0.0195\n",
      "Start position: 60\n",
      "End position: 66\n",
      "\n",
      " Note: Low confidence score suggests the model is not well-suited for this task without fine-tuning.\n",
      "EXPERIMENT 3 COMPLETE\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the risks?\"\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "print(\"EXPERIMENT 3: QUESTION ANSWERING\")\n",
    "print(f\"\\nQuestion: '{question}'\")\n",
    "print(f\"Context: '{context}'\\n\")\n",
    "\n",
    "for model_name, model_id in models.items():\n",
    "    print(f\"Testing: {model_name} ({model_id})\") \n",
    "    try:\n",
    "        qa_pipeline = pipeline('question-answering', model=model_id)\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        print(f\"Answer: '{result['answer']}'\")\n",
    "        print(f\"Confidence score: {result['score']:.4f}\")\n",
    "        print(f\"Start position: {result['start']}\")\n",
    "        print(f\"End position: {result['end']}\")\n",
    "    \n",
    "        if result['score'] < 0.1:\n",
    "            print(f\"\\n Note: Low confidence score suggests the model is not well-suited for this task without fine-tuning.\")      \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {type(e).__name__}\")\n",
    "        print(f\"Message: {str(e)[:200]}...\" if len(str(e)) > 200 else f\"Message: {str(e)}\")\n",
    "print(\"EXPERIMENT 3 COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737f64c-04a1-4277-b029-59a83e5e57de",
   "metadata": {},
   "source": [
    "| Task       | Model   | Classification (Success/Failure) | Observation (What actually happened?)                          | Why did this happen? (Architectural Reason)                          |\n",
    "|------------|---------|----------------------------------|----------------------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| Generation | BERT    | Failure                          | Generated nonsense or random symbols.                          | BERT is an encoder-only model; it isn't trained to predict the next word autoregressively. |\n",
    "|            | RoBERTa | Failure                          | Similar to BERT, produced poor or nonsensical text.            | RoBERTa is also encoder-only; despite training improvements, it cannot generate sequential text. |\n",
    "|            | BART    | Works / Suboptimal               | Could generate text but not very coherent or fluent.           | BART is encoder-decoder; supports seq2seq generation but is optimized for denoising, not pure generation. |\n",
    "| Fill-Mask  | BERT    | Success                          | Predicted words like 'create', 'generate'.                     | BERT is trained on Masked Language Modeling (MLM), making it strong at fill-mask tasks. |\n",
    "|            | RoBERTa | Success                          | Predicted contextually accurate words with better quality.     | RoBERTa improves on BERT’s MLM training with more data and no NSP, boosting fill-mask performance. |\n",
    "|            | BART    | Success                          | Correctly filled masked tokens in sentences.                   | BART’s denoising pretraining includes token masking and reconstruction, so it handles fill-mask well. |\n",
    "| QA         | BERT    | Poor Quality                     | Could answer but often inaccurate or incomplete.               | BERT can perform QA with fine-tuning, but lacks native span extraction capability. |\n",
    "|            | RoBERTa | Poor Quality                     | Similar to BERT, answers lacked precision.                     | RoBERTa shares encoder-only limitations; QA requires fine-tuning for better span prediction. |\n",
    "|            | BART    | Failure / Poor                   | Struggled to extract correct answers from passages.            | BART is not specialized for extractive QA; without fine-tuning, it cannot reliably locate spans. |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
